import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.mixture import GaussianMixture
import joblib
import os

# ===== CONFIG =====
EXCEL_PATH = '/kaggle/input/audio-prediction-dataset/excel_spectrogram1.xlsx'
BATCH_SIZE = 32
EPOCHS = 40
LEARNING_RATE = 1e-4
PATIENCE = 7
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== LOAD AND ENCODE DATA =====
df = pd.read_excel(EXCEL_PATH).dropna()

encoders = {}
for col in ['age', 'gender', 'accent']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    encoders[col] = le
    joblib.dump(le, f"{col}_encoder.pkl")

train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['accent'])

# ===== DATASET CLASS =====
class AudioDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = Image.open(row['filename_spectrogram']).convert("RGB")
        if self.transform:
            image = self.transform(image)
        age = int(row['age'])
        gender = int(row['gender'])
        accent = int(row['accent'])
        label = torch.tensor([age, gender, accent], dtype=torch.long)
        return image, label

# ===== TRANSFORMS =====
transform = transforms.Compose([
    transforms.Resize((300, 300)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# ===== LOADERS =====
train_dataset = AudioDataset(train_df, transform=transform)
val_dataset = AudioDataset(val_df, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

# ===== MODEL =====
class MultiOutputEfficientNet(nn.Module):
    def __init__(self, num_age, num_gender, num_accent):
        super(MultiOutputEfficientNet, self).__init__()
        self.base_model = models.efficientnet_b3(weights='IMAGENET1K_V1')
        in_features = self.base_model.classifier[1].in_features
        self.base_model.classifier = nn.Identity()
        self.dropout = nn.Dropout(0.5)
        self.fc_age = nn.Linear(in_features, num_age)
        self.fc_gender = nn.Linear(in_features, num_gender)
        self.fc_accent = nn.Linear(in_features, num_accent)

    def forward(self, x):
        features = self.base_model(x)
        features = self.dropout(features)
        return self.fc_age(features), self.fc_gender(features), self.fc_accent(features), features

# ===== LOSS FUNCTION =====
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.1):
        super(LabelSmoothingLoss, self).__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.cls = classes
        self.log_softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x, target):
        logprobs = self.log_softmax(x)
        with torch.no_grad():
            true_dist = torch.zeros_like(logprobs)
            true_dist.fill_(self.smoothing / (self.cls - 1))
            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))

# ===== INIT =====
model = MultiOutputEfficientNet(
    num_age=len(encoders['age'].classes_),
    num_gender=len(encoders['gender'].classes_),
    num_accent=len(encoders['accent'].classes_)
).to(DEVICE)

criterion_age = LabelSmoothingLoss(len(encoders['age'].classes_))
criterion_gender = LabelSmoothingLoss(len(encoders['gender'].classes_))
criterion_accent = LabelSmoothingLoss(len(encoders['accent'].classes_))

optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

# ===== TRAINING LOOP =====
best_val_acc = 0
patience_counter = 0
train_acc_list, val_acc_list = [], []
train_loss_list, val_loss_list = [], []

# Final epoch feature store for GMM
final_epoch_features = []
final_epoch_targets = []

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    epoch_features = []
    epoch_targets = []

    for X, y in train_loader:
        X, y = X.to(DEVICE), y.to(DEVICE)
        optimizer.zero_grad()
        out_age, out_gender, out_accent, features = model(X)

        loss = (criterion_age(out_age, y[:, 0]) +
                criterion_gender(out_gender, y[:, 1]) +
                criterion_accent(out_accent, y[:, 2]))
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        correct += ((out_age.argmax(1) == y[:, 0]).sum().item() +
                    (out_gender.argmax(1) == y[:, 1]).sum().item() +
                    (out_accent.argmax(1) == y[:, 2]).sum().item())
        total += y.size(0) * 3

        if epoch == EPOCHS - 1:  # Only collect in final epoch
            epoch_features.append(features.detach().cpu())
            epoch_targets.append(y.detach().cpu())

    avg_train_acc = correct / total
    train_loss_list.append(total_loss / len(train_loader))
    train_acc_list.append(avg_train_acc)

    # Validation
    model.eval()
    val_correct = 0
    val_total = 0
    val_loss = 0
    with torch.no_grad():
        for X, y in val_loader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            out_age, out_gender, out_accent, _ = model(X)

            loss = (criterion_age(out_age, y[:, 0]) +
                    criterion_gender(out_gender, y[:, 1]) +
                    criterion_accent(out_accent, y[:, 2]))
            val_loss += loss.item()
            val_correct += ((out_age.argmax(1) == y[:, 0]).sum().item() +
                            (out_gender.argmax(1) == y[:, 1]).sum().item() +
                            (out_accent.argmax(1) == y[:, 2]).sum().item())
            val_total += y.size(0) * 3

    avg_val_acc = val_correct / val_total
    val_loss_list.append(val_loss / len(val_loader))
    val_acc_list.append(avg_val_acc)

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Acc: {avg_train_acc:.4f} | Val Acc: {avg_val_acc:.4f}")

    if avg_val_acc > best_val_acc:
        best_val_acc = avg_val_acc
        patience_counter = 0
        torch.save(model.state_dict(), "efficientnet_gmm_hybrid_best.pth")
        print("‚úÖ Best model saved.")
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print("‚èπÔ∏è Early stopping.")
            break

    scheduler.step()

    # Save final epoch features
    if epoch == EPOCHS - 1:
        final_epoch_features = torch.cat(epoch_features)
        final_epoch_targets = torch.cat(epoch_targets)

# ===== GMM CLUSTERING =====
print("\nüîç Running GMM on extracted features...")

features_np = final_epoch_features.numpy()

# Fit and save GMM
gmm = GaussianMixture(n_components=5, random_state=42)
gmm.fit(features_np)
joblib.dump(gmm, "gmm_model.pkl")

print("‚úÖ GMM model fitted and saved.")

# ===== PLOTS =====
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_acc_list, label="Train Acc")
plt.plot(val_acc_list, label="Val Acc")
plt.legend(); plt.title("Accuracy")

plt.subplot(1, 2, 2)
plt.plot(train_loss_list, label="Train Loss")
plt.plot(val_loss_list, label="Val Loss")
plt.legend(); plt.title("Loss")
plt.tight_layout(); plt.show()



from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# ===== LOAD BEST MODEL =====
model.load_state_dict(torch.load("efficientnet_gmm_hybrid_best.pth"))
model.eval()

# ===== COLLECT PREDICTIONS AND TARGETS =====
all_preds = []
all_targets = []

with torch.no_grad():
    for X, y in val_loader:
        X = X.to(DEVICE)
        y = y.to(DEVICE)
        out_age, out_gender, out_accent, _ = model(X)

        pred_age = out_age.argmax(dim=1)
        pred_gender = out_gender.argmax(dim=1)
        pred_accent = out_accent.argmax(dim=1)

        preds = torch.stack([pred_age, pred_gender, pred_accent], dim=1)
        all_preds.append(preds.cpu())
        all_targets.append(y.cpu())

all_preds = torch.cat(all_preds, dim=0).numpy()
all_targets = torch.cat(all_targets, dim=0).numpy()

# ===== FLATTEN FOR OVERALL METRICS =====
flattened_preds = all_preds.flatten()
flattened_targets = all_targets.flatten()

# ===== METRICS =====
overall_accuracy = accuracy_score(flattened_targets, flattened_preds)
overall_precision = precision_score(flattened_targets, flattened_preds, average='macro', zero_division=0)
overall_recall = recall_score(flattened_targets, flattened_preds, average='macro', zero_division=0)
overall_f1 = f1_score(flattened_targets, flattened_preds, average='macro', zero_division=0)

# ===== PRINT =====
print(f"\nüìä Overall Validation Metrics")
print(f"‚úÖ Accuracy : {overall_accuracy:.4f}")
print(f"‚úÖ Precision: {overall_precision:.4f}")
print(f"‚úÖ Recall   : {overall_recall:.4f}")
print(f"‚úÖ F1 Score : {overall_f1:.4f}")
