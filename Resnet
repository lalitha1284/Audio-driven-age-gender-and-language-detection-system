import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib
import os

# ==== CONFIG ====
EXCEL_PATH = '/kaggle/input/audio-prediction-dataset/excel_spectrogram1.xlsx'
BATCH_SIZE = 32
EPOCHS = 25
LEARNING_RATE = 1e-4
PATIENCE = 5
WEIGHT_DECAY = 1e-4  # Added weight decay
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==== LOAD AND ENCODE DATA ====
df = pd.read_excel(EXCEL_PATH).dropna()

encoders = {}
for col in ['age', 'gender', 'accent']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    encoders[col] = le
    joblib.dump(le, f"{col}_encoder.pkl")  # Save encoder to file

# Split using only 'accent' for stratification
train_df, val_df = train_test_split(
    df, test_size=0.2, random_state=42, stratify=df['accent']
)

# ==== DATASET CLASS ====
class AudioDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = Image.open(row['filename_spectrogram']).convert("RGB")
        if self.transform:
            image = self.transform(image)
        age = int(row['age'])
        gender = int(row['gender'])
        accent = int(row['accent'])
        label = torch.tensor([age, gender, accent], dtype=torch.long)
        return image, label

# ==== TRANSFORMS ====
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ==== DATA LOADERS ====
train_dataset = AudioDataset(train_df, transform=train_transform)
val_dataset = AudioDataset(val_df, transform=val_transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ==== MODEL ====
class MultiOutputResNet(nn.Module):
    def __init__(self, num_age, num_gender, num_accent):
        super(MultiOutputResNet, self).__init__()
        self.base_model = models.resnet50(weights='IMAGENET1K_V1')
        in_features = self.base_model.fc.in_features
        self.base_model.fc = nn.Identity()

        # Add Dropout layer to prevent overfitting
        self.dropout = nn.Dropout(p=0.5)

        self.fc_age = nn.Linear(in_features, num_age)
        self.fc_gender = nn.Linear(in_features, num_gender)
        self.fc_accent = nn.Linear(in_features, num_accent)

    def forward(self, x):
        x = self.base_model(x)
        x = self.dropout(x)  # Apply dropout
        return self.fc_age(x), self.fc_gender(x), self.fc_accent(x)

model = MultiOutputResNet(
    num_age=len(encoders['age'].classes_),
    num_gender=len(encoders['gender'].classes_),
    num_accent=len(encoders['accent'].classes_)
).to(DEVICE)

# ==== LOSS FUNCTION ====
criterion_age = nn.CrossEntropyLoss()
criterion_gender = nn.CrossEntropyLoss()
criterion_accent = nn.CrossEntropyLoss()

# ==== OPTIMIZER WITH WEIGHT DECAY ====
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5, verbose=True)

# ==== TRAINING LOOP ====
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []
best_val_acc = 0
patience_counter = 0

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0
    correct_age = correct_gender = correct_accent = total = 0

    for X, y in train_loader:
        X, y = X.to(DEVICE), y.to(DEVICE)
        optimizer.zero_grad()
        
        # Get outputs from the model
        out_age, out_gender, out_accent = model(X)
        
        # Apply CrossEntropyLoss for each output (age, gender, accent)
        loss_age = criterion_age(out_age, y[:, 0])
        loss_gender = criterion_gender(out_gender, y[:, 1])
        loss_accent = criterion_accent(out_accent, y[:, 2])
        
        # Combine the losses
        loss = loss_age + loss_gender + loss_accent
        
        loss.backward()
        optimizer.step()
        
        # Track losses and accuracy
        train_loss += loss.item()
        correct_age += (out_age.argmax(1) == y[:, 0]).sum().item()
        correct_gender += (out_gender.argmax(1) == y[:, 1]).sum().item()
        correct_accent += (out_accent.argmax(1) == y[:, 2]).sum().item()
        total += y.size(0)

    avg_train_acc = (correct_age + correct_gender + correct_accent) / (3 * total)
    train_losses.append(train_loss / len(train_loader))
    train_accuracies.append(avg_train_acc)
    print(f"Epoch {epoch+1} - Loss: {train_losses[-1]:.4f} | Overall Train Acc: {avg_train_acc:.4f}")

    # Validation
    model.eval()
    val_loss = 0
    val_correct_age = val_correct_gender = val_correct_accent = val_total = 0
    with torch.no_grad():
        for X, y in val_loader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            out_age, out_gender, out_accent = model(X)

            loss_age = criterion_age(out_age, y[:, 0])
            loss_gender = criterion_gender(out_gender, y[:, 1])
            loss_accent = criterion_accent(out_accent, y[:, 2])

            val_loss += loss_age.item() + loss_gender.item() + loss_accent.item()
            val_correct_age += (out_age.argmax(1) == y[:, 0]).sum().item()
            val_correct_gender += (out_gender.argmax(1) == y[:, 1]).sum().item()
            val_correct_accent += (out_accent.argmax(1) == y[:, 2]).sum().item()
            val_total += y.size(0)

    avg_val_acc = (val_correct_age + val_correct_gender + val_correct_accent) / (3 * val_total)
    val_losses.append(val_loss / len(val_loader))
    val_accuracies.append(avg_val_acc)
    print(f"Validation Accuracy: {avg_val_acc:.4f}")

    scheduler.step(avg_val_acc)

    if avg_val_acc > best_val_acc:
        best_val_acc = avg_val_acc
        torch.save(model.state_dict(), "resnet50_audio_best_model0103.pth")
        patience_counter = 0
        print("‚úÖ Model saved with improved validation accuracy.")
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print("‚èπÔ∏è Early stopping triggered.")
            break

# Save final model
torch.save(model.state_dict(), "resnet50_audio_final_model0103.pth")

# ==== PLOT ONLY TRAIN LOSS AND TRAIN ACCURACY ====
plt.figure(figsize=(10, 4))

# Plot Train Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_accuracies, label='Train Accuracy', color='blue')
plt.title('Train Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_losses, label='Train Loss', color='red')
plt.title('Train Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig("training_curves.png")  # ‚úÖ Save the plot as image
plt.show()





import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import joblib
import pandas as pd
from torchvision import models, transforms
from sklearn.model_selection import train_test_split
from PIL import Image

# ==== CONFIG ====
EXCEL_PATH = '/kaggle/input/audio-prediction-dataset/excel_spectrogram1.xlsx'
BATCH_SIZE = 32
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==== LOAD AND ENCODE DATA ====
df = pd.read_excel(EXCEL_PATH).dropna()

# Encoding labels if not already encoded
encoders = {}
for col in ['age', 'gender', 'accent']:
    if df[col].dtype == object:  # Only encode if not already encoded
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        encoders[col] = le
        joblib.dump(le, f"{col}_encoder.pkl")  # Save encoder to file

# Split using only 'accent' for stratification
train_df, val_df = train_test_split(
    df, test_size=0.2, random_state=42, stratify=df['accent']
)

# ==== DATASET CLASS ====
class AudioDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = Image.open(row['filename_spectrogram']).convert("RGB")
        if self.transform:
            image = self.transform(image)
        age = int(row['age'])
        gender = int(row['gender'])
        accent = int(row['accent'])
        label = torch.tensor([age, gender, accent], dtype=torch.long)
        return image, label

# ==== TRANSFORMS ====
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ==== DATA LOADERS ====
val_dataset = AudioDataset(val_df, transform=val_transform)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ==== MODEL ====
class MultiOutputResNet(nn.Module):
    def __init__(self, num_age, num_gender, num_accent):
        super(MultiOutputResNet, self).__init__()
        self.base_model = models.resnet50(weights='IMAGENET1K_V1')
        in_features = self.base_model.fc.in_features
        self.base_model.fc = nn.Identity()

        # Add Dropout layer to prevent overfitting
        self.dropout = nn.Dropout(p=0.5)

        self.fc_age = nn.Linear(in_features, num_age)
        self.fc_gender = nn.Linear(in_features, num_gender)
        self.fc_accent = nn.Linear(in_features, num_accent)

    def forward(self, x):
        x = self.base_model(x)
        x = self.dropout(x)  # Apply dropout
        return self.fc_age(x), self.fc_gender(x), self.fc_accent(x)

# Load trained model
model = MultiOutputResNet(
    num_age=len(encoders['age'].classes_),
    num_gender=len(encoders['gender'].classes_),
    num_accent=len(encoders['accent'].classes_)
).to(DEVICE)

model.load_state_dict(torch.load("resnet50_audio_best_model0103.pth"))
model.eval()

# ==== METRICS FUNCTION ====
def compute_metrics(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
    return accuracy, precision, recall, f1

# ==== VALIDATION LOOP ====
all_preds_age, all_preds_gender, all_preds_accent = [], [], []
all_labels_age, all_labels_gender, all_labels_accent = [], [], []

with torch.no_grad():
    for X, y in val_loader:
        X, y = X.to(DEVICE), y.to(DEVICE)
        out_age, out_gender, out_accent = model(X)

        # Get predictions and true labels for each output
        preds_age = out_age.argmax(1).cpu().numpy()
        preds_gender = out_gender.argmax(1).cpu().numpy()
        preds_accent = out_accent.argmax(1).cpu().numpy()

        labels_age = y[:, 0].cpu().numpy()
        labels_gender = y[:, 1].cpu().numpy()
        labels_accent = y[:, 2].cpu().numpy()

        # Store the predictions and labels
        all_preds_age.extend(preds_age)
        all_preds_gender.extend(preds_gender)
        all_preds_accent.extend(preds_accent)
        all_labels_age.extend(labels_age)
        all_labels_gender.extend(labels_gender)
        all_labels_accent.extend(labels_accent)

print("\n Testing Metrics:")
print(f" Accuracy: {overall_acc:.4f}, \n Precision: {overall_prec:.4f},\n Recall: {overall_rec:.4f}, \n F1 Score: {overall_f1:.4f}")




import torch
import torchaudio
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import joblib
import librosa
import librosa.display
import numpy as np
import os

# Load encoders
age_encoder = joblib.load("age_encoder.pkl")
gender_encoder = joblib.load("gender_encoder.pkl")
accent_encoder = joblib.load("accent_encoder.pkl")

# Image transform (same as training)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# Device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model
model = MultiOutputResNet(
    num_age=len(age_encoder.classes_),
    num_gender=len(gender_encoder.classes_),
    num_accent=len(accent_encoder.classes_)
)
model.load_state_dict(torch.load("resnet50_audio_best_model0103.pth"))
model.to(DEVICE)
model.eval()

# --- Function to convert audio to spectrogram image ---
def audio_to_spectrogram_image(audio_path, output_path="temp_spec.png"):
    y, sr = librosa.load(audio_path, sr=22050)
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
    S_DB = librosa.power_to_db(S, ref=np.max)

    plt.figure(figsize=(3, 3))
    librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel')
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
    plt.close()
    return output_path

# --- Prediction function ---
def predict_audio_file(audio_path):
    # Convert to spectrogram image
    image_path = audio_to_spectrogram_image(audio_path)

    # Open and transform image
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0).to(DEVICE)

    # Inference
    with torch.no_grad():
        out_age, out_gender, out_accent = model(image)
        pred_age = age_encoder.inverse_transform(out_age.argmax(1).cpu().numpy())[0]
        pred_gender = gender_encoder.inverse_transform(out_gender.argmax(1).cpu().numpy())[0]
        pred_accent = accent_encoder.inverse_transform(out_accent.argmax(1).cpu().numpy())[0]

    print(f"üé§ Predicted Age Group: {pred_age}")
    print(f"üé§ Predicted Gender:    {pred_gender}")
    print(f"üé§ Predicted Accent:    {pred_accent}")

    # Clean up temp image
    if os.path.exists(image_path):
        os.remove(image_path)

# Example usage
predict_audio_file("/kaggle/input/audio-prediction-dataset/valid-train/valid-train/sample-000005.mp3")  # change this to your test file path
